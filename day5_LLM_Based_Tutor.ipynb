{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86517d73",
   "metadata": {},
   "source": [
    "<H1> LLM Tutor Chat</H1><br>\n",
    "<H3> User should be able to ask questions and the tool queries LLM and returns the results </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed69523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d18368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1635127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Ollama, you can use the following constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "MODEL_GPT= \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a553aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the question to be answered\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a7c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying GPT to answer with streaming\n",
    "\n",
    "def get_answer(question: str, model: str = MODEL_GPT):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": \"You are a helpful assistant that answers questions\"},\n",
    "            {\"role\":\"user\",\"content\":question}\n",
    "        ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd22840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question: str, model: str = MODEL_GPT):\n",
    "    if \"llama\" in model:\n",
    "        print(\"Using Ollama for model:\", model)\n",
    "        url = OLLAMA_API\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        }\n",
    "        response = requests.post(url, headers=HEADERS, json=data)\n",
    "        try:\n",
    "            result = response.json()['message']['content']\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            # Try to parse the first JSON object if multiple are present\n",
    "            first_json = response.text.split('\\n')[0]\n",
    "            result = json.loads(first_json)['message']['content']\n",
    "        display(Markdown(result))\n",
    "    else:\n",
    "        print(\"Using OpenAI for model:\", model)\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\": \"You are a helpful assistant that answers questions\"},\n",
    "                {\"role\":\"user\",\"content\":question}\n",
    "            ],\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18a304f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI for model: gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The line of code you provided is a Python statement that utilizes the `yield from` syntax in conjunction with a set comprehension. Let's break it down step by step to understand its functionality and purpose.\n",
       "\n",
       "1. **Set Comprehension**:\n",
       "   - The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. \n",
       "   - It iterates over a collection `books`, which is expected to be a list (or any iterable) of dictionaries (or similar objects).\n",
       "   - For each `book` in `books`, it attempts to retrieve the value associated with the key `\"author\"` using the `get` method. This method returns `None` if the key does not exist.\n",
       "   - The `if book.get(\"author\")` part is a condition that filters the authors: it ensures that only books with a non-`None` author are included in the resulting set.\n",
       "   - The result is a set of unique author names (or `None` values if they were present) from the `books` collection.\n",
       "\n",
       "2. **Yielding Results**:\n",
       "   - The `yield from` statement is used within a generator function. It allows the function to yield all values from an iterable (in this case, the set created by the set comprehension) one by one.\n",
       "   - This means that if this line is part of a generator function, calling that function would result in yielding each unique author's name to the calling context until there are no more authors left to yield.\n",
       "\n",
       "### Summary\n",
       "In summary, this line of code:\n",
       "- Creates a set of unique author names from a list of books, filtering out any books that do not have an author.\n",
       "- If part of a generator function, it allows the function to yield each unique author name one at a time, facilitating iteration over those names.\n",
       "\n",
       "### Example\n",
       "For example, if `books` were defined as:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book A\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book B\", \"author\": \"Author 2\"},\n",
       "    {\"title\": \"Book C\"},  # No author\n",
       "    {\"title\": \"Book D\", \"author\": \"Author 1\"},  # Duplicate author\n",
       "]\n",
       "```\n",
       "The resulting set comprehension would yield:\n",
       "```python\n",
       "{\"Author 1\", \"Author 2\"}\n",
       "```\n",
       "And if this was part of a generator function, it would allow you to loop through these authors one by one."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answer(question, model=MODEL_GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698ad86",
   "metadata": {},
   "source": [
    "<H1> Getting the answer using stream </H1>\n",
    "<b>change this so that the results stream back from OpenAI, with the familiar typewriter animation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7346a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_stream(question: str, model: str = MODEL_GPT):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": \"You are a helpful assistant that answers questions\"},\n",
    "            {\"role\":\"user\",\"content\":question}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"),display_id=True)\n",
    "    for chunk in stream:\n",
    "       response += chunk.choices[0].delta.content or ''\n",
    "       response = response.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "       update_display(Markdown(response),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68061c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The line of code you've provided uses Python's generator feature along with a set comprehension. Let's break it down step by step:\n",
       "\n",
       "1. **Set Comprehension**: The part inside the curly braces `{ ... for book in books if book.get(\"author\") }` is a set comprehension. This creates a set of unique values based on the criteria defined within the comprehension:\n",
       "   - `book` is iterating over a collection called `books`.\n",
       "   - `book.get(\"author\")` retrieves the value associated with the key `\"author\"` from each `book` dictionary.\n",
       "   - The `if book.get(\"author\")` condition ensures that only books that have a non-null (truthy) author value are included in the set.\n",
       "\n",
       "   So the set comprehension collects the authors of the books, but only those books that have a valid `\"author\"` entry.\n",
       "\n",
       "2. **Yield from**: The `yield from` keyword is used in Python to yield all values from an iterable. In this case, it yields each author from the set created by the comprehension. By using `yield from`, it allows the function (which this line is likely a part of) to yield each item one at a time, rather than returning the entire set at once.\n",
       "\n",
       "3. **Purpose**: The overall effect of this line is to create a generator that yields each unique author from the `books` collection, skipping any books that do not have an author listed. The use of `yield from` efficiently produces each value without loading all the values into memory at once, which is beneficial when dealing with larger datasets.\n",
       "\n",
       "### Example\n",
       "\n",
       "For instance, if you have a list of books structured like this:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\", \"author\": None},\n",
       "    {\"title\": \"Book 4\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "\n",
       "The set comprehension would create the set `{\"Author A\", \"Author B\"}` (note that `Author A` only appears once due to the uniqueness property of sets). The resulting code would yield `Author A` and then `Author B` one after the other when the generator is iterated over. \n",
       "\n",
       "### Summary\n",
       "\n",
       "In conclusion, this line of code efficiently extracts and yields unique authors from a list of book dictionaries, filtering out entries without a valid author value."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answer_stream(question, model=MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1e3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
